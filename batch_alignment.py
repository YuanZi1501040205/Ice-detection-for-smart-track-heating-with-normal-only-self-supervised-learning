import cv2
import numpy as np
import os
import imutils
import pathlib
import sys
# iteration 1
MAX_FEATURES = 500
GOOD_MATCH_PERCENT = 0.35
# #%% iteration 2
# MAX_FEATURES = 500
# GOOD_MATCH_PERCENT = 0.8

def alignImages(im1, im2):
    """
       A function to registrate raw images: transform im1 to adopt im2

       ...

       Attributes
       ----------
       img : source image (im1) and target image (im2)

       Methods
       -------
       ORB (Oriented FAST and rotated BRIEF) feature points detection and matching

        Returns
        ------
        im1Reg : transformed source image (im1)
        h : homography matrix
       """

    # Convert images to grayscale
    im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)
    im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)

    # Detect ORB features and compute descriptors.
    orb = cv2.ORB_create(MAX_FEATURES, scoreType=cv2.ORB_FAST_SCORE)
    # orb = cv2.xfeatures2d.SIFT_create(MAX_FEATURES)
    keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, None)
    keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, None)

    # Match features.
    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)
    matches = matcher.match(descriptors1, descriptors2, None)

    # Sort matches by the score
    matches.sort(key=lambda x: x.distance, reverse=False)

    # Remove not so good matches
    numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)
    matches = matches[:numGoodMatches]

    # Draw top matches
    imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)
    cv2.imwrite("matches.jpg", imMatches)

    # Extract location of good matches
    points1 = np.zeros((len(matches), 2), dtype=np.float32)
    points2 = np.zeros((len(matches), 2), dtype=np.float32)

    for i, match in enumerate(matches):
        points1[i, :] = keypoints1[match.queryIdx].pt
        points2[i, :] = keypoints2[match.trainIdx].pt

    # Find homography matrix
    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)

    # Use homography matrix
    height, width, channels = im2.shape
    im1Reg = cv2.warpPerspective(im1, h, (width, height))


    return im1Reg, h





def align_imgs(refimg, name_img, raw_img_path):
    """ based on the background subtracted image to crop the raw image's biggest object

          Inputs:
          path_ref_img: path of reference image
          name_img: raw image needed to be aligned
          raw_img_path: folder stores raw images
          ----------
          Returns: greyscale aligned image ready to be extracted.
          ------
          Notice: reference images are generated by using hand picking feature points matching first
          sample images and FLD image.
          """

    img_name = name_img
    print('img_name: ', img_name)
    ref_img = cv2.imread(refimg)
    img = cv2.imread(raw_img_path + img_name + '.jpg')
    print('img:',ref_img.shape)
    imReg, h = alignImages(img, ref_img)
    # evaluation of alignment
    flag, similar_score_before, similar_score_after =eval_align_hash(ref_img, img, imReg)

    # if flag!=True:
    #     reason = img_name + " alignment failed with similarity value(before|after): " + str(similar_score_before) + '|' + str(similar_score_after) + "# expect after >= before"
    #     raise ValueError(reason)
    # else:
    #     print("Alignment success! similarity convert from "+ str(similar_score_before)+' to '+str(similar_score_after))
    return imReg





# Average hash
def aHash(img):
    """Average hash algorithm"""
    img=cv2.resize(img,(8,8))
    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    np_mean = np.mean(gray)                           # mean of gray image
    ahash_01 = (gray>np_mean)+0                       # pixel >mean=>1 otherwise =>0
    ahash_list = ahash_01.reshape(1,-1)[0].tolist()   # flatten->2list
    ahash_str = ''.join([str(x) for x in ahash_list])
    return ahash_str


def pHash(img):
    """Perceptual hash algorithm"""
    img = cv2.resize(img, (32, 32))    #default interpolation=cv2.INTER_CUBIC
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    dct = cv2.dct(np.float32(gray))
    dct_roi = dct[0:8, 0:8]            # mask

    avreage = np.mean(dct_roi)
    phash_01 = (dct_roi>avreage)+0
    phash_list = phash_01.reshape(1,-1)[0].tolist()
    phash_str = ''.join([str(x) for x in phash_list])
    return phash_str


def hammingDist(s1, s2):
    assert len(s1) == len(s2)
    return sum([ch1 != ch2 for ch1, ch2 in zip(s1, s2)])

# get similarity of images by calculating the histogram from each channel
def classify_hist_with_split(image1, image2, size=(256, 256)):
    # resize image，seperate to RGB 3 channels ，calculate the similarity for each channel
    image1 = cv2.resize(image1, size)
    image2 = cv2.resize(image2, size)
    sub_image1 = cv2.split(image1)
    sub_image2 = cv2.split(image2)
    sub_data = 0
    for im1, im2 in zip(sub_image1, sub_image2):
        sub_data += calculate(im1, im2)
    sub_data = sub_data / 3
    return sub_data


# single channel's histogram similarity
def calculate(image1, image2):
    hist1 = cv2.calcHist([image1], [0], None, [256], [0.0, 255.0])
    hist2 = cv2.calcHist([image2], [0], None, [256], [0.0, 255.0])
    # Calculate the coincidence of the histogram
    degree = 0
    for i in range(len(hist1)):
        if hist1[i] != hist2[i]:
            degree = degree + (1 - abs(hist1[i] - hist2[i]) / max(hist1[i], hist2[i]))
        else:
            degree = degree + 1
    degree = degree / len(hist1)
    return degree



def eval_align_hash(ref_img, img_before_align, img_after_align):
    """input: bgr reference image, image before alignment, image after alignment
    return: flag(TRU/FALSE) to indicate if the alignment is success
    hash_2_score_after:float(0-1) indicate the hash similarity score between aligned image and reference image, should
    be a value close to 1 like: 0.9.
    """
    # hash method 1, average hash
    ahash_str1 = aHash(ref_img)
    ahash_str2 = aHash(img_before_align)
    ahash_str3 = aHash(img_after_align)
    # hash method 2, perceptual hash
    phash_str1 = pHash(ref_img)
    phash_str2 = pHash(img_before_align)
    phash_str3 = pHash(img_after_align)
    # similar score before alignment
    hash_1_score_before = 1 - hammingDist(ahash_str1, ahash_str2) * 1. / (32 * 32 / 4)
    hash_2_score_before = 1 - hammingDist(phash_str1, phash_str2) * 1. / (32 * 32 / 4)
    # similar score after alignment
    hash_1_score_after = 1 - hammingDist(ahash_str1, ahash_str3) * 1. / (32 * 32 / 4)
    hash_2_score_after = 1 - hammingDist(phash_str1, phash_str3) * 1. / (32 * 32 / 4)
    # similarity diff
    hash_1_score_diff = hash_1_score_after - hash_1_score_before
    hash_2_score_diff = hash_2_score_after - hash_2_score_before
    # flag of success of alignment
    if  hash_2_score_diff>=0: # hash_1_score_diff>=0 and
        flag = True
    else:
        flag = False

    return flag, hash_2_score_before, hash_2_score_after



if __name__ == "__main__":


    refimg = '/home/yzi/Proposal/code/ice_detection/ref/imagesimg1049.jpg'
    loaddir = '/home/yzi/Proposal/code/ice_detection/test/'
    savedir = '/home/yzi/Proposal/code/ice_detection/result/'

    EXTENSION = '.jpg'

    # Get a list of the images in the loading directory
    imgs_full = os.listdir(loaddir)
    print('Processing ' + str(len(imgs_full)) + ' number of images in the current folder')
    print(savedir)
    # Loop around the sample images
    for img in imgs_full:
        # Read images using OpenCV and normalize into grayscale
        name_img = img.split(EXTENSION)[0]
        print('name_img:',name_img)
        aligned_rgb_img = align_imgs(refimg, name_img, loaddir)
        cv2.imwrite(savedir + name_img + EXTENSION , aligned_rgb_img)



